{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SparkSession from builder\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('chapter01_to_05') \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015 =  spark.read.option(\"inferSchema\",\"true\").\\\n",
    "                  option(\"header\",\"true\").\\\n",
    "                  csv(\"spark_practice/datas/flight-data/csv/2015-summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData2015.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData2015.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [count#19 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(count#19 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=45]\n",
      "      +- FileScan csv [DEST_COUNTRY_NAME#17,ORIGIN_COUNTRY_NAME#18,count#19] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/fm-pc-lt-342/Documents/Spark Docx/spark_practice/datas/flig..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData2015.sort('count').explain()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle Default Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'200'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get('spark.sql.shuffle.partitions')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repartition to 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.shuffle.partitions',\"5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|             Algeria|      United States|    4|\n",
      "|              Angola|      United States|   15|\n",
      "|            Anguilla|      United States|   41|\n",
      "| Antigua and Barbuda|      United States|  126|\n",
      "|           Argentina|      United States|  180|\n",
      "|               Aruba|      United States|  346|\n",
      "|           Australia|      United States|  329|\n",
      "|             Austria|      United States|   62|\n",
      "|          Azerbaijan|      United States|   21|\n",
      "|             Bahrain|      United States|   19|\n",
      "|            Barbados|      United States|  154|\n",
      "|             Belgium|      United States|  259|\n",
      "|              Belize|      United States|  188|\n",
      "|             Bermuda|      United States|  183|\n",
      "|             Bolivia|      United States|   30|\n",
      "|Bonaire, Sint Eus...|      United States|   58|\n",
      "|              Brazil|      United States|  853|\n",
      "|British Virgin Is...|      United States|  107|\n",
      "|            Bulgaria|      United States|    3|\n",
      "|        Burkina Faso|      United States|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"This code in PySpark sorts the DataFrame flightData2015 by the column named 'DEST_COUNTRY_NAME' in ascending order and then displays the first 20 rows of the resulting DataFrame\\n. The sort() function is used to sort one or more columns in a DataFrame, and the show() function is used to display the resulting DataFrame\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015.sort('DEST_COUNTRY_NAME').show()\n",
    "\"\"\"This code in PySpark sorts the DataFrame flightData2015 by the column named 'DEST_COUNTRY_NAME' in ascending order and then displays the first 20 rows of the resulting DataFrame\n",
    ". The sort() function is used to sort one or more columns in a DataFrame, and the show() function is used to display the resulting DataFrame\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Any dataFrame can be made into a table or view with a simple method called **createOrReplaceTempView**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015.createOrReplaceTempView(\"2015_temp_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|   DEST_COUNTRY_NAME|count(1)|\n",
      "+--------------------+--------+\n",
      "|             Moldova|       1|\n",
      "|             Bolivia|       1|\n",
      "|             Algeria|       1|\n",
      "|Turks and Caicos ...|       1|\n",
      "|            Pakistan|       1|\n",
      "|    Marshall Islands|       1|\n",
      "|            Suriname|       1|\n",
      "|              Panama|       1|\n",
      "|         New Zealand|       1|\n",
      "|             Liberia|       1|\n",
      "+--------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlWay = spark.sql(\"SELECT DEST_COUNTRY_NAME, count(1) FROM 2015_temp_view GROUP BY DEST_COUNTRY_NAME\")\n",
    "sqlWay.show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to Spark Types (Literals)\n",
    "Sometimes, we need to pass explicit values into Spark that are just a value (rather than a newcolumn). This might be a constant value or something we’ll need to compare to later on. The\n",
    "way we do this is through literals. This is basically a translation from a given programming language’s literal value to one that Spark understands. Literals are expressions and you can use\n",
    "them in the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+---+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|One|\n",
      "+--------------------+-------------------+-----+---+\n",
      "|       United States|            Romania|   15|  1|\n",
      "|       United States|            Croatia|    1|  1|\n",
      "|       United States|            Ireland|  344|  1|\n",
      "|               Egypt|      United States|   15|  1|\n",
      "|       United States|              India|   62|  1|\n",
      "|       United States|          Singapore|    1|  1|\n",
      "|       United States|            Grenada|   62|  1|\n",
      "|          Costa Rica|      United States|  588|  1|\n",
      "|             Senegal|      United States|   40|  1|\n",
      "|             Moldova|      United States|    1|  1|\n",
      "|       United States|       Sint Maarten|  325|  1|\n",
      "|       United States|   Marshall Islands|   39|  1|\n",
      "|              Guyana|      United States|   64|  1|\n",
      "|               Malta|      United States|    1|  1|\n",
      "|            Anguilla|      United States|   41|  1|\n",
      "|             Bolivia|      United States|   30|  1|\n",
      "|       United States|           Paraguay|    6|  1|\n",
      "|             Algeria|      United States|    4|  1|\n",
      "|Turks and Caicos ...|      United States|  230|  1|\n",
      "|       United States|          Gibraltar|    1|  1|\n",
      "+--------------------+-------------------+-----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "literals = flightData2015.select(\"*\",lit(1).alias(\"One\")).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Columns\n",
    "PySpark withColumn() is a transformation function of DataFrame which is used to change the value, convert the datatype of an existing column, create a new column, and many more. In this post, I will walk you through commonly used PySpark DataFrame column operations using withColumn() examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+----------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|new_column|\n",
      "+-----------------+-------------------+-----+----------+\n",
      "|    United States|            Romania|   15|     false|\n",
      "|    United States|            Croatia|    1|     false|\n",
      "|    United States|            Ireland|  344|     false|\n",
      "|            Egypt|      United States|   15|     false|\n",
      "|    United States|              India|   62|     false|\n",
      "+-----------------+-------------------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, col\n",
    "adding_new_column = flightData2015.withColumn(\"new_column\",expr(\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\"))\n",
    "adding_new_column.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|new_column_2|\n",
      "+-----------------+-------------------+-----+------------+\n",
      "|    United States|            Romania|   15|          30|\n",
      "|    United States|            Croatia|    1|           2|\n",
      "|    United States|            Ireland|  344|         688|\n",
      "|            Egypt|      United States|   15|          30|\n",
      "|    United States|              India|   62|         124|\n",
      "+-----------------+-------------------+-----+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_col = flightData2015.withColumn(\"new_column_2\",col(\"count\") * 2)\n",
    "new_col.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming Columns\n",
    "Though we cannot rename a column using withColumn, still we wanted to cover this as renaming is one of the common operations we perform on DataFrame. To rename an existing column use withColumnRenamed() function on DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|Count * 2|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|    United States|            Romania|   15|       30|\n",
      "|    United States|            Croatia|    1|        2|\n",
      "|    United States|            Ireland|  344|      688|\n",
      "|            Egypt|      United States|   15|       30|\n",
      "|    United States|              India|   62|      124|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "renaming_column = new_col.withColumnRenamed(\"new_column_2\",\"Count * 2\")\n",
    "renaming_column.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Sensitivity\n",
    "By default Spark is case insensitive; however, you can make Spark case sensitive by setting the configuration:\n",
    "\n",
    "-- in SQL\n",
    "\n",
    "set spark.sql.caseSensitive true"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Columns\n",
    "df.drop(\"ORIGIN_COUNTRY_NAME\")\n",
    "\n",
    "We can drop multiple columns by passing in multiple columns as arguments:\n",
    "\n",
    "df.drop(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "drop_column = renaming_column.drop(col(\"`Count * 2`\"))\n",
    "drop_column.show(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing a Column’s Type (cast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "drop_column.withColumn(\"count\",col(\"count\").cast(\"long\")).printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Rows\n",
    "To filter rows, we create an expression that evaluates to true or false. There are two methods to perform this operation: we can use where or filter\n",
    "and they both will perform the same operation and accept the same argument types when used\n",
    "with DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData2015.filter(flightData2015[\"count\"]<2).show(3)\n",
    "# flightData2015.filter(col(\"count\") < 2).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData2015.where(col(\"count\")>5).show(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "Instinctually, you might want to put multiple filters into the same expression. Although this is possible, it is not always useful, because Spark automatically performs all filtering operations at\n",
    "the same time regardless of the filter ordering. This means that if you want to specify multiple\n",
    "AND filters, just chain them sequentially and let Spark handle the rest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|          Singapore|    1|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData2015.where(col(\"count\") < 2).where(col(\"ORIGIN_COUNTRY_NAME\") != \"Croatia\").show(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Unique Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015.select(\"DEST_COUNTRY_NAME\",\"ORIGIN_COUNTRY_NAME\").distinct().count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "|           Cyprus|      United States|    1|\n",
      "|         Djibouti|      United States|    1|\n",
      "|        Indonesia|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# flightData2015.sort(\"count\").show(5)\n",
    "# flightData2015.orderBy(\"count\", \"DEST_COUNTRY_NAME\").show(5)\n",
    "flightData2015.orderBy(col(\"count\"), col(\"DEST_COUNTRY_NAME\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|          Moldova|      United States|    1|\n",
      "|    United States|            Croatia|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|    United States|      United States|370002|\n",
      "|    United States|             Canada|  8483|\n",
      "+-----------------+-------------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To more explicitly specify sort direction, you need to use the asc and desc functions if operating\n",
    "# on a column. These allow you to specify the order in which a given column should be sorted:\n",
    "from pyspark.sql.functions import desc, asc\n",
    "\n",
    "flightData2015.orderBy(expr(\"count desc\")).show(2)\n",
    "flightData2015.orderBy(col(\"count\").desc(), col(\"DEST_COUNTRY_NAME\").asc()).show(2)\n",
    "\n",
    "\n",
    "# An advanced tip is to use asc_nulls_first, desc_nulls_first, asc_nulls_last, or\n",
    "# desc_nulls_last to specify where you would like your null values to appear in an ordered\n",
    "# DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015.rdd.getNumPartitions() # 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: int]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015.repartition(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect, Take and Show\n",
    "\n",
    "collect() : collect gets all data from the entire DataFrame.\n",
    "\n",
    "take(N) : take gets the first N rows.\n",
    "\n",
    "show() : how prints out a number of rows nicely."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
